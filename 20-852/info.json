{
    "abstract": "There has been a surge of recent interest in graph representation learning (GRL). GRL methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding, focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between network embedding, graph regularization and graph neural networks. We propose a comprehensive taxonomy of GRL methods, aiming to unify several disparate bodies of work. Specifically, we propose the GraphEDM framework, which generalizes popular algorithms for semi-supervised learning (e.g. GraphSage, GCN, GAT), and unsupervised learning (e.g. DeepWalk, node2vec) of graph representations into a single consistent approach. To illustrate the generality of GraphEDM, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",
    "authors": [
        "Ines Chami",
        "Sami Abu-El-Haija",
        "Bryan Perozzi",
        "Christopher R\u00e9",
        "Kevin Murphy"
    ],
    "emails": [
        "chami@stanford.edu",
        "sami@haija.org",
        "bperozzi@acm.org",
        "chrismre@cs.stanford.edu",
        "kpmurphy@google.com"
    ],
    "id": "20-852",
    "issue": 89,
    "pages": [
        1,
        64
    ],
    "title": "Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
    "volume": 23,
    "year": 2022
}